# Attention

Attention was initially designed in the context of Neural Machine Translation using Seq2Seq Models.

Attention takes 2 sentences and turns them into a matrix.
The words of the first sentence for the columns, the words of the second sentence form the rows. Then it makes matches, identifying relevant context. 

